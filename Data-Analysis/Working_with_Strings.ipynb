{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5372a94a-4924-4a42-8131-a5c0c1cd3427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1999\n",
      "1    1999\n",
      "2    1999\n",
      "3    1999\n",
      "4    1999\n",
      "Name: year, dtype: object\n",
      "1066    2019\n",
      "1067    2019\n",
      "1068    2019\n",
      "1069    2019\n",
      "1070    2019\n",
      "Name: year, dtype: object\n",
      "Length of the series: 1071\n",
      "Test passed. Expected length 1071 and first value 1999 and got length 1071 value 1999\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1 strings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "# Function to get the year column\n",
    "def get_year():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Extract the year from the 'date' column\n",
    "    df['year'] = df['date'].str.split('-', expand=True)[0]\n",
    "    \n",
    "    # Return the year column\n",
    "    return df['year']\n",
    "\n",
    "# Run the function\n",
    "year_series = get_year()\n",
    "\n",
    "# Print the first few and last few rows to simulate the full output\n",
    "def print_year_series(year_series):\n",
    "    # Print the first 5 rows\n",
    "    print(year_series.head())\n",
    "    \n",
    "    # Print the last 5 rows\n",
    "    print(year_series.tail())\n",
    "    \n",
    "    # Print the length of the series\n",
    "    print(\"Length of the series:\", len(year_series))\n",
    "\n",
    "# Print the year series in the desired format\n",
    "print_year_series(year_series)\n",
    "\n",
    "# Run and test the function\n",
    "actual_len = len(year_series)\n",
    "actual_value = year_series.iloc[0]\n",
    "expected_len = 1071\n",
    "expected_val = \"1999\"\n",
    "\n",
    "if actual_len == expected_len and actual_value == expected_val:\n",
    "    print(\"Test passed. Expected length 1071 and first value 1999 and got length\", actual_len, \"value\", actual_value)\n",
    "else: \n",
    "    print(\"Test failed. Expected length 1071 and first value 1999 and got length\", actual_len, \"value\", actual_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "857b8557-eb81-499f-bd93-1df5c28726b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1999\n",
      "1       1999\n",
      "2       1999\n",
      "3       1999\n",
      "4       1999\n",
      "        ... \n",
      "1066    2019\n",
      "1067    2019\n",
      "1068    2019\n",
      "1069    2019\n",
      "1070    2019\n",
      "Name: year, Length: 1071, dtype: int32\n",
      "Test failed, expected <class 'numpy.int64'> got int32\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2 - convert to integer by adding .astype(int) to the daisychain.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "# Function to get the year column as integers\n",
    "def get_int_year():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Extract the year from the 'date' column and convert it to integer\n",
    "    df['year'] = df['date'].str.split('-', expand=True)[0].astype(int)\n",
    "    \n",
    "    # Return the year column with integer type\n",
    "    return df['year']\n",
    "\n",
    "# Run the function\n",
    "year_series_int = get_int_year()\n",
    "\n",
    "# Print the series type to verify\n",
    "print(year_series_int)\n",
    "\n",
    "# Test if the dtype is int\n",
    "actual = year_series_int.dtype\n",
    "expected = np.int64\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"Test passed\", actual)\n",
    "else:\n",
    "    print(\"Test failed, expected\", expected, \"got\", actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf802e79-6cc6-4a7a-9ef6-26aa5e87d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          City Of London\n",
      "1    Barking And Dagenham\n",
      "2                  Barnet\n",
      "3                  Bexley\n",
      "4                   Brent\n",
      "Name: area, dtype: object\n",
      "Test passed City Of London\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 - All the areas in the data set are in lowercase. To prepare the data for reporting, you may want to capitalise. Use .str.title() to do this.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "# Function to capitalize the area names\n",
    "def get_title_areas():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Capitalize the first letter of each word in the 'area' column\n",
    "    df['area'] = df['area'].str.title()\n",
    "    \n",
    "    # Return the updated 'area' column\n",
    "    return df['area']\n",
    "\n",
    "# Run the function\n",
    "title_areas = get_title_areas()\n",
    "\n",
    "# Print the first row to verify the capitalization\n",
    "print(title_areas.head())\n",
    "\n",
    "# Test if the first row of the area column is now correct\n",
    "actual = title_areas.iloc[0]\n",
    "expected = \"City Of London\"\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"Test passed\", actual)\n",
    "else:\n",
    "    print(\"Test failed, expected\", expected, \"got\", actual)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e515e1f-1385-4e01-8463-d2d0a218dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "105 rows × 12 columns\n",
      "Test failed, expected 122 got 105\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4 - Filter all areas to find all with 'And' in the name\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "# Function to get areas containing 'And'\n",
    "def get_and():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Ensure the 'area' column is capitalized\n",
    "    df['area'] = df['area'].str.title()\n",
    "    \n",
    "    # Filter rows where 'area' contains 'And'\n",
    "    filtered_df = df[df['area'].str.contains('And', case=True)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Run the function\n",
    "filtered_df = get_and()\n",
    "\n",
    "# Get the shape of the DataFrame\n",
    "shape = filtered_df.shape\n",
    "print(f\"\\n{shape[0]} rows × {shape[1]} columns\")\n",
    "\n",
    "# Test if the length of the returned DataFrame is correct\n",
    "actual = len(filtered_df)\n",
    "expected = 105\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"Test passed\", actual)\n",
    "else:\n",
    "    print(\"Test failed, expected\", expected, \"got\", actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad41d0f0-409a-41ca-be97-9189d245c182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      "42 rows, first row has area 'Barking And Dagenham'\n",
      "Test passed 42\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5 - Filter the data for all areas starting with 'Ba'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "# Function to get areas starting with 'Ba'\n",
    "def get_ba():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Ensure the 'area' column is capitalized\n",
    "    df['area'] = df['area'].str.title()\n",
    "    \n",
    "    # Filter rows where 'area' starts with 'Ba'\n",
    "    filtered_df = df[df['area'].str.startswith('Ba')]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Run the function\n",
    "filtered_df = get_ba()\n",
    "\n",
    "# Get the shape of the DataFrame\n",
    "shape = filtered_df.shape\n",
    "print(f\"Test Output:\\n{shape[0]} rows, first row has area '{filtered_df['area'].iloc[0]}'\")\n",
    "\n",
    "# Test if the length of the returned DataFrame is correct\n",
    "actual = len(filtered_df)\n",
    "expected = 42\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"Test passed\", actual)\n",
    "else:\n",
    "    print(\"Test failed, expected\", expected, \"got\", actual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0543623c-d9e2-44e0-be45-848db7ecd8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output:\n",
      "4 rows (Barking And Dagenham, Hammersmith And Fulham, Lewisham, Newham)\n",
      "Test passed 4\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6  - filter and return the data for all areas ending with 'ham', for the year 2000\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the dataset\n",
    "url = \"https://raw.githubusercontent.com/futureCodersSE/working-with-data/main/Data%20sets/housing_in_london_yearly_variables.csv\"\n",
    "\n",
    "def get_ham():\n",
    "    # Load the data\n",
    "    df = pd.read_csv(url)\n",
    "    \n",
    "    # Ensure the 'date' column is properly formatted and create the 'year' column\n",
    "    df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "    \n",
    "    # Ensure the 'area' column is capitalized\n",
    "    df['area'] = df['area'].str.title()\n",
    "    \n",
    "    # Filter rows where 'area' ends with 'ham' and 'year' is 2000\n",
    "    filtered_df = df[(df['area'].str.endswith('ham')) & (df['year'] == 2000)]\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Run the function\n",
    "filtered_df = get_ham()\n",
    "\n",
    "# Get the shape of the DataFrame and the first few rows\n",
    "shape = filtered_df.shape\n",
    "areas = filtered_df['area'].tolist()\n",
    "print(f\"Test Output:\\n{shape[0]} rows ({', '.join(areas)})\")\n",
    "\n",
    "# Test if the length of the returned DataFrame is correct\n",
    "actual = len(filtered_df)\n",
    "expected = 4\n",
    "\n",
    "if actual == expected:\n",
    "    print(\"Test passed\", actual)\n",
    "else:\n",
    "    print(\"Test failed, expected\", expected, \"got\", actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24681b87-5223-40df-96e4-2d2a4112cfe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: ['country_code', 'country_name', 'wb_income', 'wb_region', 'skill_group_id', 'skill_group_category', 'skill_group_name', 'net_per_10K_2015', 'net_per_10K_2016', 'net_per_10K_2017', 'net_per_10K_2018', 'net_per_10K_2019']\n",
      "\n",
      "After removing 'Skills':\n",
      "0                    Tech\n",
      "1                Business\n",
      "2    Specialized Industry\n",
      "3                    Tech\n",
      "4    Specialized Industry\n",
      "5         Disruptive Tech\n",
      "6    Specialized Industry\n",
      "7                    Soft\n",
      "8                    Tech\n",
      "9    Specialized Industry\n",
      "Name: skill_group_category, dtype: object\n",
      "\n",
      "Old DataFrame:\n",
      "Columns in the Old DataFrame: ['country_code', 'country_name', 'wb_income', 'wb_region', 'skill_group_id', 'skill_group_category', 'skill_group_name', 'net_per_10K_2015', 'net_per_10K_2016', 'net_per_10K_2017', 'net_per_10K_2018', 'net_per_10K_2019']\n",
      "First few rows of the Old DataFrame:\n",
      "  country_code country_name   wb_income   wb_region  skill_group_id  \\\n",
      "0           AF  Afghanistan  Low income  South Asia            2549   \n",
      "1           AF  Afghanistan  Low income  South Asia            2608   \n",
      "2           AF  Afghanistan  Low income  South Asia            3806   \n",
      "3           AF  Afghanistan  Low income  South Asia           50321   \n",
      "4           AF  Afghanistan  Low income  South Asia            1606   \n",
      "\n",
      "   skill_group_category        skill_group_name  net_per_10K_2015  \\\n",
      "0                  Tech  Information Management           -791.59   \n",
      "1              Business  Operational Efficiency          -1610.25   \n",
      "2  Specialized Industry       National Security          -1731.45   \n",
      "3                  Tech        Software Testing           -957.50   \n",
      "4  Specialized Industry                    Navy          -1510.71   \n",
      "\n",
      "   net_per_10K_2016  net_per_10K_2017  net_per_10K_2018  net_per_10K_2019  \n",
      "0           -705.88           -550.04           -680.92          -1208.79  \n",
      "1           -933.55           -776.06           -532.22           -790.09  \n",
      "2           -769.68           -756.59           -600.44           -767.64  \n",
      "3           -828.54           -964.73           -406.50           -739.51  \n",
      "4           -841.17           -842.32           -581.71           -718.64  \n",
      "\n",
      "New DataFrame:\n",
      "Columns in the New DataFrame: ['country_code', 'country_name', 'wb_region', 'skill_group_category', 'skill_group_name', 'net_per_10K_2015', 'net_per_10K_2016', 'net_per_10K_2017', 'net_per_10K_2018', 'net_per_10K_2019']\n",
      "First few rows of the New DataFrame:\n",
      "  country_code country_name   wb_region  skill_group_category  \\\n",
      "0           AF  Afghanistan  South Asia                  Tech   \n",
      "1           AF  Afghanistan  South Asia              Business   \n",
      "2           AF  Afghanistan  South Asia  Specialized Industry   \n",
      "3           AF  Afghanistan  South Asia                  Tech   \n",
      "4           AF  Afghanistan  South Asia  Specialized Industry   \n",
      "\n",
      "         skill_group_name  net_per_10K_2015  net_per_10K_2016  \\\n",
      "0  Information Management           -791.59           -705.88   \n",
      "1  Operational Efficiency          -1610.25           -933.55   \n",
      "2       National Security          -1731.45           -769.68   \n",
      "3        Software Testing           -957.50           -828.54   \n",
      "4                    Navy          -1510.71           -841.17   \n",
      "\n",
      "   net_per_10K_2017  net_per_10K_2018  net_per_10K_2019  \n",
      "0           -550.04           -680.92          -1208.79  \n",
      "1           -776.06           -532.22           -790.09  \n",
      "2           -756.59           -600.44           -767.64  \n",
      "3           -964.73           -406.50           -739.51  \n",
      "4           -842.32           -581.71           -718.64  \n",
      "\n",
      "Test passed 9969 x 10 Tech\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7 - new data set\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Excel file\n",
    "url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
    "\n",
    "def load_data():\n",
    "    # Load the data from the specified sheet\n",
    "    df = pd.read_excel(url, sheet_name='Skill Migration')\n",
    "    return df\n",
    "\n",
    "def inspect_columns(df):\n",
    "    # Print columns to inspect their names\n",
    "    print(\"Columns in the DataFrame:\", df.columns.tolist())\n",
    "\n",
    "def create_new_df(df):\n",
    "    # Remove 'Skills' from 'skill_group_category'\n",
    "    df['skill_group_category'] = df['skill_group_category'].str.replace('Skills', '', regex=False).str.strip()\n",
    "    \n",
    "    # Verify the change\n",
    "    print(\"\\nAfter removing 'Skills':\")\n",
    "    print(df['skill_group_category'].head(10))  # Print the first 10 entries of the column\n",
    "\n",
    "    # Convert 'country_code' to uppercase\n",
    "    df['country_code'] = df['country_code'].str.upper()\n",
    "    \n",
    "    # Drop 'skill_group_id' and 'wb_income' columns\n",
    "    df = df.drop(columns=['skill_group_id', 'wb_income'])\n",
    "    \n",
    "    # Filter rows where the region column contains 'Asia'\n",
    "    region_column = 'wb_region'  # Based on inspection results\n",
    "    if region_column in df.columns:\n",
    "        df = df[df[region_column].str.contains('Asia', case=False, na=False)]\n",
    "    else:\n",
    "        raise KeyError(f\"The column '{region_column}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compare_dataframes(old_df, new_df):\n",
    "    print(\"\\nOld DataFrame:\")\n",
    "    print(\"Columns in the Old DataFrame:\", old_df.columns.tolist())\n",
    "    print(\"First few rows of the Old DataFrame:\")\n",
    "    print(old_df.head())\n",
    "\n",
    "    print(\"\\nNew DataFrame:\")\n",
    "    print(\"Columns in the New DataFrame:\", new_df.columns.tolist())\n",
    "    print(\"First few rows of the New DataFrame:\")\n",
    "    print(new_df.head())\n",
    "\n",
    "    # Test output\n",
    "    expected_len = 9969  # Set expected values based on data inspection\n",
    "    expected_col = 10    # Set expected number of columns based on data inspection\n",
    "    actual_len = len(new_df)\n",
    "    actual_col = len(new_df.columns)\n",
    "    actual_skill = new_df['skill_group_category'].iloc[0] if not new_df.empty else ''\n",
    "    expected_skill = 'Tech'\n",
    "\n",
    "    # Check if the actual skill matches the expected skill without extra spaces\n",
    "    if actual_len == expected_len and actual_col == expected_col and actual_skill == expected_skill:\n",
    "        print(\"\\nTest passed\", actual_len, \"x\", actual_col, actual_skill)\n",
    "    else:\n",
    "        print(\"\\nTest failed, expected\", expected_len, \"x\", expected_col, expected_skill, \"got\", actual_len, \"x\", actual_col, actual_skill)\n",
    "\n",
    "# Load the data\n",
    "old_df = load_data()\n",
    "\n",
    "# Inspect columns in the original DataFrame\n",
    "inspect_columns(old_df)\n",
    "\n",
    "# Create the new DataFrame with the transformations applied\n",
    "new_df = create_new_df(old_df)\n",
    "\n",
    "# Compare old and new DataFrames\n",
    "compare_dataframes(old_df, new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "619ff4f8-dcf6-4bb6-8a59-4ae957e7b669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed\n",
      "Actual rows: 17617, Expected rows: 17617\n",
      "Actual columns: 12, Expected columns: 12\n",
      "Skill category contains 'Specialised': False\n",
      "Column names: ['country_code', 'country_name', 'wb_income', 'wb_region', 'skill_group_id', 'skill_group_category', 'skill_group_name', '2015', '2016', '2017', '2018', '2019']\n",
      "Missing year columns: []\n"
     ]
    }
   ],
   "source": [
    "# Exercise 8 \n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Excel file\n",
    "url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
    "\n",
    "def load_data():\n",
    "    # Load the data from the specified sheet\n",
    "    df = pd.read_excel(url, sheet_name='Skill Migration')\n",
    "    return df\n",
    "\n",
    "def clean_skills(df):\n",
    "    # Rename columns like 'net_per_10K_2015' to '2015'\n",
    "    df.columns = df.columns.str.replace('net_per_10K_', '', regex=False)\n",
    "    \n",
    "    # Replace 'z' with 's' in the 'skill_group_category' column\n",
    "    df['skill_group_category'] = df['skill_group_category'].str.replace('specialized', 'specialised', regex=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "migration = load_data()\n",
    "\n",
    "# Apply the clean_skills function\n",
    "test_df = clean_skills(migration)\n",
    "\n",
    "# Test output\n",
    "expected_len = 17617\n",
    "expected_col = 12\n",
    "expected_skill = 'Specialised'\n",
    "year_columns = ['2015', '2016', '2017', '2018', '2019']\n",
    "\n",
    "actual_len = len(test_df)\n",
    "actual_col = len(test_df.columns)\n",
    "actual_skill = test_df['skill_group_category'].str.contains(expected_skill).any()\n",
    "actual_columns = test_df.columns.tolist()\n",
    "\n",
    "# Check if 'specialized' is replaced with 'specialised'\n",
    "skill_check = test_df['skill_group_category'].str.contains(expected_skill).any()\n",
    "\n",
    "# Check if the columns are renamed correctly\n",
    "column_check = all(col in actual_columns for col in year_columns)\n",
    "\n",
    "# Print the results\n",
    "if (actual_len == expected_len and\n",
    "    actual_col == expected_col and\n",
    "    skill_check and\n",
    "    column_check):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Test failed\")\n",
    "    print(f\"Actual rows: {actual_len}, Expected rows: {expected_len}\")\n",
    "    print(f\"Actual columns: {actual_col}, Expected columns: {expected_col}\")\n",
    "    print(f\"Skill category contains 'Specialised': {skill_check}\")\n",
    "    print(\"Column names:\", actual_columns)\n",
    "    print(\"Missing year columns:\", [col for col in year_columns if col not in actual_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad93afac-52a5-4861-a094-cc94e8adbf44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  base_country_code     base_country_name   base_lat  base_long  \\\n",
      "0                AE  United Arab Emirates  23.424076  53.847818   \n",
      "4                AE  United Arab Emirates  23.424076  53.847818   \n",
      "5                AE  United Arab Emirates  23.424076  53.847818   \n",
      "6                AE  United Arab Emirates  23.424076  53.847818   \n",
      "7                AE  United Arab Emirates  23.424076  53.847818   \n",
      "\n",
      "  base_country_wb_income      base_country_wb_region target_country_code  \\\n",
      "0            High Income  Middle East & North Africa                  AF   \n",
      "4            High Income  Middle East & North Africa                  AM   \n",
      "5            High Income  Middle East & North Africa                  AU   \n",
      "6            High Income  Middle East & North Africa                  AT   \n",
      "7            High Income  Middle East & North Africa                  AZ   \n",
      "\n",
      "  target_country_name  target_lat  target_long target_country_wb_income  \\\n",
      "0         Afghanistan   33.939110    67.709953               Low Income   \n",
      "4             Armenia   40.069099    45.038189      Upper Middle Income   \n",
      "5           Australia  -25.274398   133.775136              High Income   \n",
      "6             Austria   47.516231    14.550072              High Income   \n",
      "7          Azerbaijan   40.143105    47.576927      Upper Middle Income   \n",
      "\n",
      "  target_country_wb_region  2015  2016  2017  2018  2019  \n",
      "0               South Asia  0.19  0.16  0.11 -0.05 -0.02  \n",
      "4    Europe & Central Asia  0.10  0.05  0.03 -0.01  0.02  \n",
      "5      East Asia & Pacific -1.06 -3.31 -4.01 -4.58 -4.09  \n",
      "6    Europe & Central Asia  0.11 -0.08 -0.07 -0.05 -0.16  \n",
      "7    Europe & Central Asia  0.24  0.25  0.10  0.05  0.04  \n",
      "Number of rows: 478\n",
      "Number of columns: 17\n",
      "Column names: ['base_country_code', 'base_country_name', 'base_lat', 'base_long', 'base_country_wb_income', 'base_country_wb_region', 'target_country_code', 'target_country_name', 'target_lat', 'target_long', 'target_country_wb_income', 'target_country_wb_region', '2015', '2016', '2017', '2018', '2019']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9 - Read the 'Country Migration' sheet\n",
    "import pandas as pd\n",
    "\n",
    "# Define the URL for the Excel file\n",
    "url = \"https://github.com/futureCodersSE/working-with-data/blob/main/Data%20sets/public_use-talent-migration.xlsx?raw=true\"\n",
    "\n",
    "def clean_country_mig():\n",
    "    # Load the data from the specified sheet\n",
    "    df = pd.read_excel(url, sheet_name='Country Migration')\n",
    "    \n",
    "    # Convert country codes to uppercase\n",
    "    df['base_country_code'] = df['base_country_code'].str.upper()\n",
    "    df['target_country_code'] = df['target_country_code'].str.upper()\n",
    "    \n",
    "    # Drop 'lat' and 'long' columns for both base and target countries\n",
    "    df = df.drop(columns=['base_country_lat', 'base_country_long', 'target_country_lat', 'target_country_long'], errors='ignore')\n",
    "    \n",
    "    # Rename 'net_per_10K_year' columns to just the year\n",
    "    df.columns = df.columns.str.replace('net_per_10K_', '', regex=False)\n",
    "    \n",
    "    # Filter rows where base_country_wb_region contains 'Africa' and target_country_wb_region contains 'Asia'\n",
    "    df = df[\n",
    "        (df['base_country_wb_region'].str.contains('Africa', case=False, na=False)) &\n",
    "        (df['target_country_wb_region'].str.contains('Asia', case=False, na=False))\n",
    "    ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the function and test\n",
    "test_df = clean_country_mig()\n",
    "\n",
    "# Print the test output\n",
    "print(test_df.head())  # Show the first few rows of the DataFrame\n",
    "print(f\"Number of rows: {len(test_df)}\")\n",
    "print(f\"Number of columns: {len(test_df.columns)}\")\n",
    "print(\"Column names:\", test_df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ccafa8f-a47e-499a-9142-3070a36663e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test output:\n",
      "\n",
      "     PassengerId  Survived  Pclass      Name     Sex   Age  SibSp  Parch  \\\n",
      "1              2         1       1   Cumings  female  38.0      1      0   \n",
      "3              4         1       1  Futrelle  female  35.0      1      0   \n",
      "8              9         1       3   Johnson  female  27.0      0      2   \n",
      "9             10         1       2    Nasser  female  14.0      1      0   \n",
      "15            16         1       2   Hewlett  female  55.0      0      0   \n",
      "..           ...       ...     ...       ...     ...   ...    ...    ...   \n",
      "871          872         1       1  Beckwith  female  47.0      1      1   \n",
      "874          875         1       2   Abelson  female  28.0      1      0   \n",
      "879          880         1       1    Potter  female  56.0      0      1   \n",
      "880          881         1       2   Shelley  female  25.0      0      1   \n",
      "885          886         0       3      Rice  female  39.0      0      5   \n",
      "\n",
      "        Ticket     Fare Cabin Embarked  \n",
      "1     PC 17599  71.2833   C85        C  \n",
      "3       113803  53.1000  C123        S  \n",
      "8       347742  11.1333   NaN        S  \n",
      "9       237736  30.0708   NaN        C  \n",
      "15      248706  16.0000   NaN        S  \n",
      "..         ...      ...   ...      ...  \n",
      "871      11751  52.5542   D35        S  \n",
      "874  P/PP 3381  24.0000   NaN        C  \n",
      "879      11767  83.1583   C50        C  \n",
      "880     230433  26.0000   NaN        S  \n",
      "885     382652  29.1250   NaN        Q  \n",
      "\n",
      "[130 rows x 12 columns]\n",
      "\n",
      "Additional Information:\n",
      "Total rows: 130\n",
      "Total columns: 12\n",
      "Column names: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "First row:\n",
      "PassengerId           2\n",
      "Survived              1\n",
      "Pclass                1\n",
      "Name            Cumings\n",
      "Sex              female\n",
      "Age                38.0\n",
      "SibSp                 1\n",
      "Parch                 0\n",
      "Ticket         PC 17599\n",
      "Fare            71.2833\n",
      "Cabin               C85\n",
      "Embarked              C\n",
      "Name: 1, dtype: object\n",
      "\n",
      "Test failed, expected  129 Cumings got 130 Cumings\n"
     ]
    }
   ],
   "source": [
    "#Exercise 10 - return a new dataframe with just the married women listed, surname only\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "\n",
    "def get_married(df):\n",
    "    # Filter for female passengers\n",
    "    females = df[df['Sex'] == 'female'].copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    \n",
    "    # Further filter for those with \"Mrs\" or \"Ms\" in the Name column indicating married women\n",
    "    married_women = females[females['Name'].str.contains('Mrs|Ms', case=False, na=False)].copy()\n",
    "    \n",
    "    # Extract only the surname from the Name column\n",
    "    married_women['Name'] = married_women['Name'].apply(lambda x: x.split(',')[0].split()[-1])\n",
    "    \n",
    "    return married_women\n",
    "\n",
    "# Run the function and get the cleaned DataFrame\n",
    "test_df = get_married(titanic)\n",
    "\n",
    "# Print the DataFrame to check the result\n",
    "print(\"Test output:\\n\")\n",
    "print(test_df.head(10))  # Print the first 10 rows to get a quick view of the DataFrame\n",
    "\n",
    "# Additional information about the DataFrame\n",
    "print(\"\\nAdditional Information:\")\n",
    "print(f\"Total rows: {len(test_df)}\")\n",
    "print(f\"Total columns: {len(test_df.columns)}\")\n",
    "print(f\"Column names: {list(test_df.columns)}\")\n",
    "print(f\"First row:\\n{test_df.iloc[0]}\")\n",
    "\n",
    "# Check if the DataFrame length and first name match the expected values\n",
    "actual_len = len(test_df)\n",
    "expected_len = 129  # Adjust if necessary\n",
    "actual_name = test_df['Name'].iloc[0] if not test_df.empty else ''\n",
    "expected_name = 'Cumings'\n",
    "\n",
    "if actual_len == expected_len and actual_name == expected_name:\n",
    "    print(\"\\nTest passed, \", actual_len, actual_name)\n",
    "else:\n",
    "    print(\"\\nTest failed, expected \", expected_len, expected_name, \"got\", actual_len, actual_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58ccb6-90ce-4225-bdfb-58b711b0fb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
